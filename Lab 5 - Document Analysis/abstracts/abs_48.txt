074.03 / VV16 - An automated emotion extractor of scenes that mimics the canonical human viewer

Emotions often drive us and our behaviors. Certain scenes, movies, stories, commercials invoke a strong visceral, emotional reaction in our brains. An ability to reliably and automatically estimate the degree and kind of emotion aroused has application in social media, business analytics, human interaction etc., i.e. any enterprise in which human behavior is an integral component. Two dimensions of emotion are key in this regard: arousal value, or the extent to which a scene excites or calms the canonical viewer, and its valence, i.e. the extent to which the scene is pleasant or unpleasant. Past attempts to automate emotion detection in images have failed for a variety of reasons, e.g. lack of images independently evaluated for emotional content, limited generality of approach, use of low-level features like color, texture etc. Our novel, integrated approach overcomes said limitations: For each image of IAPS and NAPS - standardized databases of images whose emotional content has been independently measured using human raters -, it weights several high-level semantic features, which were extracted by a deep learning network, and sums the weighted contributions to arrive at numerical values for arousal and valence. The model has two parts: a front-end consisting of a bank of classifiers, where each classifier evaluates the probability of a particular semantic category (e.g. a human) in the image; a back-end regressor that takes the front-end outputs and weights the contribution of each semantic category to generate numerical values for valence and arousal. The front-end classifiers were extensions of the deep learning network Inception_v3. Training images were obtained from google images, so the model never saw IAPS or NAPS images during training. The numerical values of valence and arousal output by the model were linearly correlated significantly (p << 0.0001 for both arousal and valence) with manual ratings, and were discretized to one of two classes for valence (high/low) and three classes for arousal (high/medium/low). Overall system performance was evaluated by comparing the model class with ground truth, i.e. the class obtained from discretizing the mean ratings provided by human raters in the IAPS and NAPS studies. Our best performing model demonstrated test accuracy of 98.0% on valence (chance = 50%) and 95.0% on arousal classification (chance = 33%) across IAPS+NAPS (n=2202 images combined). We further tested the model on GAPED - a third externally validated set of images (n=730) that were never shown or used to train the model - and it, again, exhibited high levels of accuracy: 96.4% on valence and 92.5% on arousal classification.
